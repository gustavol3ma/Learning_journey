# -*- coding: utf-8 -*-
"""RAG_AULA03ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CPFDYFD8V7RFQ3qM0CLkftlPmGFOYHwv

Embeddings de Alta Performance
"""

!pip install langchain langchain-google-genai sentence-transformers scikit-learn langchain-community

import os
import time
import numpy as np

from google.colab import userdata
os.environ["GOOGLE_API_KEY"] = userdata.get('APY_RAG')

from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from sklearn.metrics.pairwise import cosine_similarity

textos_teste = [
    "Qual é a política de férias da nossa empresa?",
    "Preciso de um relatório de despesas de viagem.",
    "Como configuro o acesso à rede privada virtual (VPN)?",
    "Onde encontro o código de conduta da organização?",
    "Quero entender o processo de avaliação de performance."
]

!pip install langchain==0.3.26 langchain-community==0.3.27 langchain-core==0.3.71 langchain-google-genai==2.1.8 sentence-transformers==4.1.0 scikit-learn==1.6.1

gemini_embeddings = GoogleGenerativeAIEmbeddings(model="gemini-embedding-001")

start_time=time.time()
embeddings_gemini = gemini_embeddings.embed_documents(textos_teste)
end_time = time.time()
print(f"Tempo de processamento: {end_time - start_time} segundos")
print(f'Dimensões de vetor: {len(embeddings_gemini[0])}')

"""Trabalhando com ALMINILML6 versão 2

Agora, vamos trabalhar com o ALMINILML6 versão 2. Em vez de repetir o processo anterior, faremos uma nova chamada utilizando o Hugging Face Embeddings. O nome do modelo será ALMINILML6v2, e o restante do processo será semelhante. Faremos as trocas necessárias e rodaremos o código.
"""

minilm_embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
start_time = time.time()
embeddings_minilm = minilm_embeddings.embed_documents(textos_teste)
end_time = time.time()

print(f"Tempo de processamento: {end_time - start_time} segundos")
print(f"Dimensões do vetor: {len(embeddings_minilm[0])}")

"""BGE-Lage

o BGE Large, também do Hugging Face. O processo será semelhante, mas utilizaremos o BGE Embeddings. Faremos as substituições necessárias e rodaremos o código.
"""

bge_embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-large-en-v1.5")

start_time = time.time()
embeddings_bge = bge_embeddings.embed_documents(textos_teste)
end_time = time.time()

print(f"Tempo de processamento: {end_time - start_time} segundos")
print(f"Dimensões do vetor: {len(embeddings_bge[0])}")

"""Análise da Qualidade Semântica"""

pergunta = 'Quero tira férias'

emb_pergunta_gemini = gemini_embeddings.embed_query(pergunta)
emb_pergunta_minilm = minilm_embeddings.embed_query(pergunta)
emb_pergunta_bge = bge_embeddings.embed_query(pergunta)

modelos = {
    "Gemini": (emb_pergunta_gemini, embeddings_gemini),
    "MiniLM": (emb_pergunta_minilm, embeddings_minilm),
    "BGE-large": (emb_pergunta_bge, embeddings_bge)
}

print(pergunta)

for nome, (emb_q, emb_docs) in modelos.items():
    similaridades = cosine_similarity([emb_q], emb_docs)[0]
    doc_e_similaridade = sorted(
        zip(textos_teste, similaridades), key=lambda x: x[1], reverse=True
    )
    print(f"--- Ranking para o modelo {nome} ---")
    for i, (doc, sim) in enumerate(doc_e_similaridade[:3], 1):
        print(f"{i}. (Score: {sim:.3f}) {doc}")
    print()

"""Caching de Embeddings

Quando trabalhamos com caching de embeds, queremos gerar embeds, especialmente via API, pois elas têm um custo em tempo e dinheiro. O caching armazena os embeds já calculados para evitar retrabalho. Vamos utilizar o Cache Embeds do LangChain e trazer algumas bibliotecas necessárias para isso.
"""

from langchain.storage import LocalFileStore
from langchain.embeddings import CacheBackedEmbeddings

store = LocalFileStore('./cache')
embedder_principal = GoogleGenerativeAIEmbeddings(model="gemini-embedding-001")
cache_embeddings = CacheBackedEmbeddings.from_bytes_store(
    embedder_principal,
    store,
    namespace='gemini_cache'
)

textos_para_cache = ["Olá, mundo!", "Testando o cache de embeddings.", "Olá, mundo!"]
start_time = time.time()
embeddings_result_1 = cache_embeddings.embed_documents(textos_para_cache)
end_time = time.time()
print(f" Tempo de execução: {end_time - start_time:.4f} segundos.")

"""**Implementando o processamento em lotes**


Batch Processing para indexação em Larga Escala

o Batch Processing para edição em larga escala. Ao editar milhares de documentos, processar em lotes é fundamental. Modelos locais se beneficiam enormemente disso. Se tivermos um documento grande, faremos um exemplo com uma iteração de mil.
"""

documentos_grandes = [f"Este é o documento de teste número {i}." for i in range(1000)]
bge_embedder = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en-v1.5",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

batch_sizes = [1, 32, 64, 128]

for batch_size in batch_sizes:
    start_time = time.time()
    num_batches = len(documentos_grandes) // batch_size
    tempo_estimado = num_batches * (0.1 * batch_size) + (len(documentos_grandes) % batch_size) * 0.1
    tempo_real = bge_embedder.client.encode(documentos_grandes, batch_size=batch_size)
    end_time = time.time()
    print(f"  - Batch Size: {batch_size:<4} -> Tempo: {end_time - start_time:.2f}s")