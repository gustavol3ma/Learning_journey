# -*- coding: utf-8 -*-
"""RAG_AULA02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OPxhVHZa30KPY6xYRYsUneAHcdnnvqzB
"""

!pip install --upgrade langchain langchain-google-genai faiss-cpu chromadb langchain-pinecone pinecone-client langchain-community

import os
from google.colab import userdata
os.environ["GOOGLE_API_KEY"] = userdata.get('APY_RAG')

from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.documents import Document

embeddings = GoogleGenerativeAIEmbeddings(model='gemini-embedding-001')
#Ela instancia um modelo de embeddings do Google Gemini(via LangChain),permitindo transformar
#textos em vetores numéricos de alta dimensão

embeddings.embed_query("Política de home office da empresa")

documentos_empresa = [
    Document(
        page_content="Política de férias: Funcionários têm direito a 30 dias de férias após 12 meses. A solicitação deve ser feita com 30 dias de antecedência.",
        metadata={"tipo": "política", "departamento": "RH", "ano": 2024, "id_doc": "doc001"}
    ),
    Document(
        page_content="Processo de reembolso de despesas: Envie a nota fiscal pelo portal financeiro. O reembolso ocorre em até 5 dias úteis.",
        metadata={"tipo": "processo", "departamento": "Financeiro", "ano": 2023, "id_doc": "doc002"}
    ),
    Document(
        page_content="Guia de TI: Para configurar a VPN, acesse vpn.nossaempresa.com e siga as instruções para seu sistema operacional.",
        metadata={"tipo": "tutorial", "departamento": "TI", "ano": 2024, "id_doc": "doc003"}
    ),
    Document(
        page_content="Código de Ética e Conduta: Valorizamos o respeito, a integridade e a colaboração. Casos de assédio não serão tolerados.",
        metadata={"tipo": "política", "departamento": "RH", "ano": 2022, "id_doc": "doc004"}
    )
]

from langchain_community.vectorstores import FAISS
import faiss

d = 768
index_hns2= faiss.IndexHNSWFlat(d,32)

faiss_db = FAISS.from_documents(documentos_empresa, embeddings)

pergunta =  "Como peço minhas férias?"
resultados = faiss_db.similarity_search_with_score(pergunta, k=2)

print(f"\n Pergunta: {pergunta}")
print("\n Documento mais relevantes (FAISS):")
for doc in resultados:
  print(f"- {doc[0].page_content}")
  print(f"(Metadados: {doc[0].metadata})")

"""Chroma DB: Banco de Dados Vetorial Simples e Poderoso para IA"""

from langchain_community.vectorstores import Chroma

chroma_db = Chroma.from_documents(
    documents = documentos_empresa,
    embedding = embeddings
)

resultados = chroma_db.similarity_search(pergunta, k=2)

for doc in resultados:
  print(f"- {doc.page_content}")

"""Pergunta por Filtro

"""

pergunta_rh = "Quais são as regras da empresa?"

resultados_filtrado = chroma_db.similarity_search(pergunta_rh, k=2, filter={"$and": [{"departamento" : "RH"},{"tipo" : "política"}]})

for doc in resultados_filtrado:
  print(f"- {doc.page_content}")
  print(f"(Metadados: {doc.metadata['departamento']}, Tipo: {doc.metadata['tipo']}")

"""Pinecone - Escalabilidade na Nuvem


 É um banco de dados vetorial de nível profissional, muito utilizado em ambientes de produção.
"""

from google.colab import userdata
userdata.get('APY_PINECONE')
os.environ['PINECONE_API_KEY'] = userdata.get('APY_PINECONE')

from langchain_community.vectorstores import Pinecone
from pinecone import Pinecone as PineconeClient
from pinecone import ServerlessSpec

index_name ="langchain-rag"
pinecone_client = PineconeClient(api_key=os.environ["PINECONE_API_KEY"])
spec = ServerlessSpec(cloud='aws', region='us-east-1')

!pip show langchain-pinecone

!pip show pinecone-client

from langchain_pinecone import Pinecone
from pinecone import Pinecone as PineconeClient
from pinecone import ServerlessSpec


index_name = "langchain-rag"
if index_name not in pinecone_client.list_indexes().names():
    pinecone_client.create_index(
        name=index_name,
        dimension=d,
        metric="cosine",
        spec=spec
    )
    print(f"Índice '{index_name}' criado no Pinecone.")

    pinecone_db = Pinecone.from_documents(
        documentos_empresa,
        embeddings,
        index_name=index_name
    )

    print(f"Documentos adicionados ao índice '{index_name}'")

else:
    print(f"Conectando ao índice existente '{index_name}'.")
    pinecone_db = Pinecone.from_existing_index(
        index_name,
        embeddings
    )

if pinecone_db:
  # Busca por similaridade
  pergunta_ti = "Como configuro a VPN?"
  resultados_pinecone = pinecone_db.similarity_search(pergunta_ti, k=2)
  print(f"\n Pergunta: {pergunta_ti}")
  print(f"\n Documentos mais relevantes (Pinecone):")
  for doc in resultados_pinecone:
    print(f"- {doc.page_content}")
    print(f"(Metadados: {doc.metadata})")

  # Busca com filtro (Pinecone também suporta)
  resultados_pinecone_filtrados = pinecone_db.similarity_search(
      "informações sobre regras",k=2,filter={"tipo": "politica"})

  print(f"\n Pergunta: 'Informações sobre regras' com filtro para tipo='politica'")
  print(f"\n Documentos relevantes e filtrados (Pinecone):")
  for doc in resultados_pinecone_filtrados:
    print(f"- {doc.page_content}")
    print(f"(Tipo: {doc.metadata['tipo']})")